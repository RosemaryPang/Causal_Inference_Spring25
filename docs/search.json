[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome Page",
    "section": "",
    "text": "Welcome to DACSS790 Causal Inference. All classroom material, including recordings, slides, tutorials, and assignment descriptions, will be posted in Canvas.\nThis tutorial website contains the R code and output of weekly tutorials.\n Weekly Tutorials \nWeek 1 Regression Review & Bias\nWeek 2 Directed Acyclic Graphs\nWeek 3 Randomization Design\nWeek 4 Mediation Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "",
    "text": "In this tutorial, you’ll learn (or refresh your memory) about hypothesis testing, ordinary least squares (OLS) regression, transformation, logistic regression, and we’ll discuss why we can’t make causal statement by simply adding control variables to our model.\nBy the end of this tutorial, you should be familiar with the following:\n1. Basic concepts of hypothesis testing\n2. OLS regression: single and multiple linear regression, regression with interaction term\n3. Transformation\n4. Logistic regression"
  },
  {
    "objectID": "Tutorial1.html#example",
    "href": "Tutorial1.html#example",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Example",
    "text": "Example\nA manufacturer says the battery of their laptop lasts 10 hours on average. We want to test whether this is true (\\(\\mu = 10\\)). Because we are interested in deviations from 10 in either direction, this is a two-sided test. Now we have a sample, with sample mean (\\(\\bar{x}\\)) = 9.04, sample standard deviation (\\({s}\\)) = 1.83, and sample size = 30. Can we reject the null hypothesis?\nWe first calculate t-value based on sample statistics:\n\\[t=\\frac{\\bar{x} - \\mu_0}{{s}/{\\sqrt{n}}}=\\frac{9.04-10}{1.83/{\\sqrt{30}}}=-2.87\\]\nWe then need to find the critical value base on our \\(\\alpha\\)-level. Let’s set \\(\\alpha=0.05\\). Since the absolute value of t-value is greater than the critical value, we can reject the null hypothesis.\n\ncritical &lt;- round(qt(0.975,df=(30-1)),2)\ncritical\n\n[1] 2.05\n\n\nWe can manually calculate the p-value. With a p-value smaller than 0.05 (the \\(\\alpha\\)-level), we can also reject the null hypothesis.\n\nt_statistic &lt;- 2.87\np_value &lt;- (1 - pt(t_statistic, df= 29)) * 2\np_value\n\n[1] 0.00758546"
  },
  {
    "objectID": "Tutorial1.html#simple-linear-regression",
    "href": "Tutorial1.html#simple-linear-regression",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nTo start, let’s consider the simplest case—a single independent variable predicting an outcome. This is known as simple linear regression, where we model a straight-line relationship between the predictor and the response variable. We will walk through an example using student.survey data.\n\ndata(student.survey)\nhead(student.survey)\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\nstudent.survey$male &lt;- ifelse(student.survey$ge == \"m\", 1, 0)\n\nThis data is from smss package. It consists of responses of graduate students in the social sciences enrolled in STA 6126 in a recent term at the University of Florida. Variables in this data include gender (ge), high school GPA (hi), average number of hours per week that you watch TV (tv), and political affiliation (pa). We start by looking into the relationship between watching TV and high school GPA.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Hours of TV Watched per Week\", \n       y = \"GPA\", \n       title = \"Relationship Between TV Watching and GPA\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe visualization tells us there’s a negative relationship between watching TV and GPA. We can use OLS regression to test whether this relationship is statistically significant.\n\nm1 &lt;- lm(hi~tv, data=student.survey)\nsummary(m1)\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nFrom the OLS regression results, we can model the relationship between hours watching TV and high school GPA: \\(hi=3.441−0.0183*tv\\). When a student watches 0 hours of TV, their predicted GPA is 3.441. Each additional hour of TV watched per week is associated with a decrease of 0.018 points in the GPA, and the impact is statistically significant at the 95% confidence level."
  },
  {
    "objectID": "Tutorial1.html#multiple-linear-regression",
    "href": "Tutorial1.html#multiple-linear-regression",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn reality, most outcomes are influenced by more than one factor. Simple linear regression is often too simplistic to capture complex relationships. Multiple linear regression extends the model by including multiple independent variables, allowing us to control for additional factors and better isolate the effect of each predictor. Let’s explore how this works with the same data.\n\nm2 &lt;- lm(hi~tv+male+pa,data=student.survey)\nsummary(m2)\n\n\nCall:\nlm(formula = hi ~ tv + male + pa, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11908 -0.25376  0.02283  0.31597  0.68353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.428977   0.133216  25.740   &lt;2e-16 ***\ntv          -0.017175   0.009004  -1.907   0.0617 .  \nmale        -0.138154   0.117482  -1.176   0.2447    \npai          0.161780   0.134692   1.201   0.2349    \npar          0.024887   0.153392   0.162   0.8717    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.446 on 55 degrees of freedom\nMultiple R-squared:  0.1223,    Adjusted R-squared:  0.0585 \nF-statistic: 1.917 on 4 and 55 DF,  p-value: 0.1206\n\n\nResults show that after controlling for gender and party affiliation, hours of TV watching still has a statistically significant impact on GPA at the 90% confidence level. Each additional hour of TV watched per week is associated with a 0.017 point decrease in GPA. Compared to Democrats, Independents have a 0.16 point higher GPA, but this difference is not statistically significant.\nIn this model, we use Democrats as the reference category (baseline) for party affiliation. You can also change the baseline into other category using relevel() function.\n\nstudent.survey$pa2 &lt;- relevel(student.survey$pa, ref = \"i\")\n\nIf we set Independents as the baseline category for party affiliation, how does being Republican affect high school GPA compared to Independents?"
  },
  {
    "objectID": "Tutorial1.html#interaction-term",
    "href": "Tutorial1.html#interaction-term",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Interaction Term",
    "text": "Interaction Term\nSo far, we have assumed that each independent variable has an independent and additive effect on the dependent variable. However, in many cases, the effect of one variable depends on the level of another. This is where interaction terms come in. By including interaction terms in our regression model, we can capture how the relationship between one variable and the outcome changes depending on another variable.\n\nm3 &lt;- lm(hi~tv*male, data=student.survey)\nsummary(m3)\n\n\nCall:\nlm(formula = hi ~ tv * male, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18709 -0.23932  0.07162  0.30540  0.74298 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.539245   0.130413  27.139   &lt;2e-16 ***\ntv          -0.021885   0.012852  -1.703   0.0941 .  \nmale        -0.177346   0.173034  -1.025   0.3098    \ntv:male      0.004405   0.017531   0.251   0.8025    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4482 on 56 degrees of freedom\nMultiple R-squared:  0.09761,   Adjusted R-squared:  0.04926 \nF-statistic: 2.019 on 3 and 56 DF,  p-value: 0.1216\n\n\nIn this model, \\(GPA=3.539−0.0219TV−0.1773Male+0.0044(TV*Male)\\). Unlike previous models where we could interpret coefficients directly, interaction terms modify the effect of one variable depending on the value of another. This means we need to substitute specific values to understand the impact on different groups. For female students (male=0), watching one additional hour of TV is associated with a 0.0219 point decrease in GPA. For male students (male=1), each additional hour of TV is associated with a 0.0175 point (\\(-0.219+0.044\\)) decrease in GPA. Since the interaction term is not statistically significant, it suggests that the relationship between TV watching and GPA is similar for both genders."
  },
  {
    "objectID": "Tutorial1.html#regression-table",
    "href": "Tutorial1.html#regression-table",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Regression Table",
    "text": "Regression Table\nTo present the results of the three regression models (m1, m2, and m3) side by side in a well-formatted table, you can use the stargazer package in R.\n\nstargazer(m1, m2, m3, type = \"text\",\n          title = \"Regression Results\",\n          dep.var.labels = \"High School GPA\",\n          covariate.labels = c(\"TV Hours\", \"Male\", \"Party Affiliation: Independent\", \"Party Affiliation: Republican\", \"TV Hours x Male\"),\n          omit.stat = c(\"f\", \"ser\"),\n          no.space = TRUE)\n\n\nRegression Results\n============================================================\n                                    Dependent variable:     \n                               -----------------------------\n                                      High School GPA       \n                                  (1)       (2)       (3)   \n------------------------------------------------------------\nTV Hours                       -0.018**   -0.017*   -0.022* \n                                (0.009)   (0.009)   (0.013) \nMale                                      -0.138    -0.177  \n                                          (0.117)   (0.173) \nParty Affiliation: Independent             0.162            \n                                          (0.135)           \nParty Affiliation: Republican              0.025            \n                                          (0.153)           \nTV Hours x Male                                      0.004  \n                                                    (0.018) \nConstant                       3.441***  3.429***  3.539*** \n                                (0.085)   (0.133)   (0.130) \n------------------------------------------------------------\nObservations                      60        60        60    \nR2                               0.072     0.122     0.098  \nAdjusted R2                      0.056     0.059     0.049  \n============================================================\nNote:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "Tutorial1.html#simpsons-paradox-and-collider-bias",
    "href": "Tutorial1.html#simpsons-paradox-and-collider-bias",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Simpson’s Paradox and Collider Bias",
    "text": "Simpson’s Paradox and Collider Bias\nSimpson’s paradox means that a tend appears in different groups of data, but disappears or reverses when these groups are combined. Collider bias occurs when we condition on (control for, subset by, or select a sample based on) a variable that is influenced by two other variables. Let’s look into an example using simulated data.\n\nset.seed(123)\n\nn &lt;- 500  \n\n# Generate exercise levels (hours per week)\nexercise &lt;- rnorm(n, mean = 5, sd = 2)\n\n# Generate blood pressure - Randomly generated, NO relationship with exercise\nblood_pressure &lt;- rnorm(n, mean = 120, sd = 5)\n\n# Obesity is influenced by both exercise and blood pressure (collider)\nobesity &lt;- ifelse(exercise &lt; 5 | blood_pressure &gt; 120, 1, 0)\n\n# Modify Blood Pressure to create group-specific effects\nblood_pressure[obesity == 0] &lt;- blood_pressure[obesity == 0] - 2 * exercise[obesity == 0]  # Exercise LOWERS BP for non-obese\nblood_pressure[obesity == 1] &lt;- blood_pressure[obesity == 1] + 2 * exercise[obesity == 1]  # Exercise RAISES BP for obese\n\n# Create dataframe\ndata &lt;- data.frame(exercise, blood_pressure, obesity)\n\nThe visualization below shows an example of Simpson’s Paradox: the trend differs when analyzing separate groups compared to the pooled data.\n\nggplot(data, aes(x = exercise, y = blood_pressure, color = as.factor(obesity))) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"Pooled Data\"), method = \"lm\", se = FALSE, data = data, linetype = \"dashed\") +\n  labs(title = \"Simpson's Paradox: Exercise & Blood Pressure with Obesity as Collider\",\n       x = \"Exercise (hours per week)\",\n       y = \"Blood Pressure (mmHg)\",\n       color = \"Obesity Status\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn our case, obesity is a collider because it is influenced by both exercise and blood pressure. Let’s examine the regression results by running models on the pooled data, within each obesity group separately, and with obesity as a control variable.\n\n# Overall regression (ignoring obesity)\noverall &lt;- lm(blood_pressure ~ exercise, data = data)\n\n# Regression within subgroup\nobese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 1))\nnon_obese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 0))\n\n# Controlling for obesity\ncontrol &lt;- lm(blood_pressure ~ exercise + obesity, data = data)\n\nstargazer(overall, obese, non_obese, control,\n          type = \"text\")\n\n\n=======================================================================================================================\n                                                            Dependent variable:                                        \n                    ---------------------------------------------------------------------------------------------------\n                                                              blood_pressure                                           \n                              (1)                     (2)                      (3)                      (4)            \n-----------------------------------------------------------------------------------------------------------------------\nexercise                   -1.593***                2.819***                -1.997***                 2.048***         \n                            (0.314)                 (0.128)                  (0.211)                  (0.135)          \n                                                                                                                       \nobesity                                                                                              32.395***         \n                                                                                                      (0.586)          \n                                                                                                                       \nConstant                  130.907***               117.935***              115.721***                88.997***         \n                            (1.705)                 (0.619)                  (1.422)                  (0.991)          \n                                                                                                                       \n-----------------------------------------------------------------------------------------------------------------------\nObservations                  500                     362                      138                      500            \nR2                           0.049                   0.575                    0.396                    0.867           \nAdjusted R2                  0.047                   0.573                    0.392                    0.866           \nResidual Std. Error    13.651 (df = 498)        4.443 (df = 360)        3.206 (df = 136)          5.112 (df = 497)     \nF Statistic         25.717*** (df = 1; 498) 486.228*** (df = 1; 360) 89.334*** (df = 1; 136) 1,618.847*** (df = 2; 497)\n=======================================================================================================================\nNote:                                                                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIn Model 1, we see that exercise has a negative impact on blood pressure, which makes sense. However, when we examine different subgroups, we find that the direction of the effect varies across groups—a classic example of Simpson’s Paradox. Surprisingly, in Model 4, after controlling for obesity, the coefficient for exercise flips from negative to positive. Does this mean that exercising regularly increases blood pressure? Of course not. This misleading result occurs because obesity is a collider, and controlling for a collider introduces bias rather than reducing it."
  },
  {
    "objectID": "Tutorial1.html#multicollinearity",
    "href": "Tutorial1.html#multicollinearity",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nAdding too many control variables can lead to multicollinearity, where some predictors become highly correlated with each other. This makes it difficult to determine the independent effect of each variable, leading to unstable estimates and inflated standard errors. As a result, even if a variable has a real effect, its coefficient may appear insignificant due to the overlap in explanatory power with other controls. Let’s look into the example below.\nIn this example, we have 100 employee with employees’ ages range from 25 to 65. Everyone in the company graduated college at the age of 22, and started working there. Everyone is paid roughly based on their experience. If we run regression using both age and experience as independent variables, we have multicollinearity problem.\n\nset.seed(123)\nage &lt;- sample(25:65, size  = 100, replace = TRUE)\nexperience &lt;- age - 22\nsalary &lt;- 2000 * (experience) + rnorm(100, sd = 20000)\n\nsummary(lm(salary ~ age + experience))\n\n\nCall:\nlm(formula = salary ~ age + experience)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40598 -12992   -983  11339  65231 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41856.8     8607.8  -4.863 4.41e-06 ***\nage           1943.2      181.7  10.694  &lt; 2e-16 ***\nexperience        NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19630 on 98 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.5338 \nF-statistic: 114.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThis situation is easy to detect because R completely omits the coefficient for experience, indicating perfect multicollinearity. However, in real-world cases, variables may be highly correlated but not perfectly related, meaning R will still report a coefficient. For example, let’s add some variation to age.\n\nage &lt;- age + round(rnorm(100)) \nsummary(lm(salary ~ age + experience))\n\n\nCall:\nlm(formula = salary ~ age + experience)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40591 -12986   -992  11336  65225 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   757.713  45204.994   0.017    0.987\nage             6.244   2049.783   0.003    0.998\nexperience   1936.977   2066.586   0.937    0.351\n\nResidual standard error: 19740 on 97 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.529 \nF-statistic: 56.59 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nNow, we see that although R reports coefficients for both variables, the results are quite strange. The standard errors are extremely high. The p-values are large, meaning we fail to detect a significant relationship – even though age and experience should clearly influence salary. However, the F-test p-value is still small, suggesting that at least one of the predictors matters – we just can’t tell which one!\nThis demonstrates two key indicators of multicollinearity: 1. Large standard errors (high uncertainty in estimates). 2. High p-values for individual variables, despite a significant overall model (low F-test p-value).\nMulticollinearity makes estimates unreliable, leading to misleading or biased results. So, be cautious when adding too many highly correlated control variables—sometimes, they do more harm than good!"
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) help us visualize causal relationships between variables. In this tutorial, you’ll learn about creating DAGs in R. By the end of this tutorial, you should be familiar with the following:\n1. packages ggdag and dagitty\n2. Create a simple DAG\n3. Customize DAG with colors, and labels\n4. Highlight causal paths, confounders, and colliders"
  },
  {
    "objectID": "Tutorial2.html#finding-paths",
    "href": "Tutorial2.html#finding-paths",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Finding paths",
    "text": "Finding paths\nggdag_paths() function shows all the causal and non-causal paths between the exposure and outcome, including frontdoor paths, backdoor paths and paths involving colliders.\n\nggdag_paths(winelife)"
  },
  {
    "objectID": "Tutorial2.html#finding-parents",
    "href": "Tutorial2.html#finding-parents",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Finding parents",
    "text": "Finding parents\nThe ggdag_parents() function identifies and visualizes the parent nodes of a selected variable in a DAG. Identifying parent nodes can help identify potential backdoor paths if the selected node is the exposure in a DAG.\n\nggdag_parents(winelife_label, \"Wine\", text=F, use_labels = \"label\")"
  },
  {
    "objectID": "Tutorial2.html#closing-the-backdoor",
    "href": "Tutorial2.html#closing-the-backdoor",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Closing the backdoor",
    "text": "Closing the backdoor\nThe backdoor criterion says that we have sufficient set of variables to control for confounding when it blocks all backdoor paths from treatment to the outcome, and when it does not include any descendants of treatment. Using ggdag_adjustment_set() function, we can quickly get the minimally sufficient adjustment sets to adjust for when analyzing the effect of x on y.\n\nggdag_adjustment_set(winelife_label,text=F,use_labels = \"label\")\n\n\n\n\n\n\n\n\nLet’s see another practice using the example on Lecture Slide page 23.\n\n# First make the DAG\nPractice &lt;- dagify(\n  Y ~ V + A + M,\n  A ~ Z + W,\n  M ~ W,\n  W ~ Z,\n  Z ~ V,\n  exposure = \"A\",\n  outcome = \"Y\"\n)\n\nggdag(Practice, layout=\"stress\")\n\n\n\n\n\n\n\nggdag_adjustment_set(Practice)"
  },
  {
    "objectID": "Tutorial2.html#control-for-collider",
    "href": "Tutorial2.html#control-for-collider",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Control for collider?",
    "text": "Control for collider?\nIn the lecture, we emphasized that we should never control/block a collider. Because doing so induces a fake correlation between its parent variables, even if they were originally independent.\nTo formally analyze whether variables are conditionally independent (d-separated) or conditionally related (d-connected) in a DAG, we can use the function ggdag_dseparated().\n\n# First create a DAG with a collider\nIncomeHealth &lt;- dagify(\n  Wine ~ Income + Health,\n  exposure = \"Income\",\n  outcome = \"Health\"\n)\n\nggdag(IncomeHealth, layout='tree')\n\n\n\n\n\n\n\nggdag_dseparated(IncomeHealth)\n\n\n\n\n\n\n\n\nWe see that Health and Income are d-separated, meaning they are independent. Let’s see what happens if we control for the collider.\n\nggdag_dseparated(IncomeHealth,\n  controlling_for = \"Wine\")\n\n\n\n\n\n\n\n\nNow we see after controlling the collider, Health and Income become d-connected, meaning we are creating a fake relationship between them."
  },
  {
    "objectID": "Tutorial2.html#blocking-the-frontdoor",
    "href": "Tutorial2.html#blocking-the-frontdoor",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Blocking the frontdoor?",
    "text": "Blocking the frontdoor?\nFrontdoor adjustment leverages the mediator as a tool to estimate the causal effect of X on Y. It does not simply ‘control for’ or ‘block’ the mediator in the same way we adjust for confounder. We will discuss frontdoor adjustment in more detail during the Mediation Analysis week. For now, let’s explore what happens when we block the mediator.\n\n# First create a DAG with a mediator\nwinedrug &lt;- dagify(\n  Lifespan ~ Drug,\n  Drug ~ Wine,\n  exposure = \"Wine\",\n  outcome = \"Lifespan\"\n)\n\nggdag(winedrug, layout='stress')\n\n\n\n\n\n\n\nggdag_dseparated(winedrug)\n\n\n\n\n\n\n\nggdag_dseparated(winedrug, controlling_for = \"Drug\")"
  },
  {
    "objectID": "Tutorial3.html",
    "href": "Tutorial3.html",
    "title": "Tutorial 3 Randomization Design",
    "section": "",
    "text": "Randomization is a fundamental principle in experimental design that helps ensure unbiased causal inference. In this tutorial, you’ll learn how to implement different randomization techniques in R using the randomizr package. By the end of this tutorial, you should be familiar with the following:\n1. packages randomizr\n2. Simple random assignment\n3. Complete random assignment\n4. Block random assignment\n5. Clustered assignment\n6. Blocked and clustered assignment\n\nFront-end Matters\nrandomizr is a lightweight R package designed to simplify the random assignment process in experiments. It provides a transparent, flexible, and reproducible way to assign units to treatment and control groups using various randomization designs. Proper randomization is essential for ensuring valid causal inference, but in many studies, the details of how treatment was assigned are often lost or imprecisely documented. randomizr helps researchers generate, document, and replicate random assignments with ease, reducing errors and improving experimental rigor.\n\n#install.packages(\"randomizr\")\n\nlibrary(randomizr)\n\n\n\nSimulating data for demonstration\nWe’ll simulate a dataset of 600 individuals, each with attributes such as FavoriteFruit, PreferredDrink, and AgeGroup. Additionally, we’ll introduce Region to represent clustering and DietType to represent blocks.\n\nset.seed(123)  \n\nN &lt;- 600\n\n# Simulate attributes\nFavoriteFruit &lt;- sample(c(\"Apple\", \"Banana\", \"Cherry\", \"Orange\"), N, replace = TRUE)\nPreferredDrink &lt;- sample(c(\"Water\", \"Juice\", \"Soda\", \"Tea\"), N, replace = TRUE)\nAgeGroup &lt;- sample(c(\"Child\", \"Teen\", \"Adult\", \"Senior\"), N, replace = TRUE)\n\n# Simulate clusters (Regions)\nRegion &lt;- sample(paste(\"Region\", 1:6), N, replace = TRUE)\n\n# Simulate blocks (Diet Types)\nDietType &lt;- sample(c(\"Vegetarian\", \"Vegan\", \"Keto\", \"Low-fat\"), N, replace = TRUE)\n\n# Combine into a data frame\nsimulated_data &lt;- data.frame(\n  FavoriteFruit,\n  PreferredDrink,\n  AgeGroup,\n  Region,\n  DietType\n)\n\n# Display the first few rows of the dataset\nhead(simulated_data)\n\n  FavoriteFruit PreferredDrink AgeGroup   Region   DietType\n1        Cherry            Tea    Child Region 2 Vegetarian\n2        Cherry          Juice   Senior Region 2    Low-fat\n3        Cherry            Tea     Teen Region 5      Vegan\n4        Banana          Water    Child Region 6    Low-fat\n5        Cherry           Soda    Child Region 5       Keto\n6        Banana           Soda    Child Region 4    Low-fat\n\n\nWe now need to create simulated potential outcomes. We’ll call the untreated outcome \\(Y0\\) and we’ll call the treated outcome \\(Y1\\). If we were really running an experiment, we would only observe either \\(Y0\\) or \\(Y1\\) for each subject, but since we are simulating, we generate both. Our inferential target is the average treatment effect (ATE), which is defined as the average difference between \\(Y0\\) and \\(Y1\\).\n\n# Convert categorical variables to factors\nsimulated_data$AgeGroup &lt;- factor(simulated_data$AgeGroup, levels = c(\"Child\", \"Teen\", \"Adult\", \"Senior\"))\nsimulated_data$DietType &lt;- factor(simulated_data$DietType, levels = c(\"Vegetarian\", \"Vegan\", \"Keto\", \"Low-fat\"))\n\n# Assign numerical values to AgeGroup and DietType for outcome calculation\nage_effect &lt;- as.numeric(simulated_data$AgeGroup)\ndiet_effect &lt;- as.numeric(simulated_data$DietType)\n\n# Calculate potential outcomes\nset.seed(123)  # For reproducibility\nsimulated_data$Y0 &lt;- rnorm(N, mean = 10 + age_effect - diet_effect, sd = 3)\nsimulated_data$Y1 &lt;- simulated_data$Y0 + 2*age_effect + 3*diet_effect \n\n# Display the first few rows with outcomes\nhead(simulated_data)\n\n  FavoriteFruit PreferredDrink AgeGroup   Region   DietType        Y0       Y1\n1        Cherry            Tea    Child Region 2 Vegetarian  8.318573 13.31857\n2        Cherry          Juice   Senior Region 2    Low-fat  9.309468 29.30947\n3        Cherry            Tea     Teen Region 5      Vegan 14.676125 24.67612\n4        Banana          Water    Child Region 6    Low-fat  7.211525 21.21153\n5        Cherry           Soda    Child Region 5       Keto  8.387863 19.38786\n6        Banana           Soda    Child Region 4    Low-fat 12.145195 26.14519\n\n# Calculate true ATE\nATE_true &lt;- with(simulated_data, mean(Y1 - Y0))\nprint(ATE_true)\n\n[1] 12.41\n\n\nWe are now ready to allocate treatment assignments to subjects. Let’s start by contrasting simple and complete random assignment.\n\n\nSimple random assignment\nSimple random assignment assigns all subjects to treatment with an equal probability by flipping a (weighted) coin for each subject. The main trouble with simple random assignment is that a different number of subjects might be assigned to each group.\nsimple_ra() assumes a two-group design and a 0.50 probability of assignment. Note that if we don’t set seed, the number of subject in each group is subject to change.\n\nset.seed(123)\nZ &lt;- simple_ra(N=N)\ntable(Z)\n\nZ\n  0   1 \n310 290 \n\n\nWe can compare the ATE from the randomized experiment, and compare it with the true ATE.\n\n# Assign treatment and control groups based on random assignment\nsimulated_data2 &lt;- cbind(simulated_data,Z)\n\nY_observed &lt;- ifelse(simulated_data2$Z == 1, simulated_data2$Y1, simulated_data2$Y0)\n\nATE_est &lt;- mean(Y_observed[Z == 1]) - mean(Y_observed[Z == 0])\nprint(ATE_est)\n\n[1] 12.02648\n\n\nThis estimated ATE is pretty close to the true ATE (12.41). This suggests that this random sample is a good representation of the population.\nWe can also change the probability of assignment, by specifying the prob argument. prob indicates the percentage that receive the treatment.\n\nZ &lt;- simple_ra(N = N, prob = 0.30)\ntable(Z)\n\nZ\n  0   1 \n425 175 \n\n\nIs this still a good representation of the population?\n\n# Assign treatment and control groups based on random assignment\nsimulated_data2 &lt;- cbind(simulated_data,Z)\n\nY_observed &lt;- ifelse(simulated_data2$Z == 1, simulated_data2$Y1, simulated_data2$Y0)\n\nATE_est &lt;- mean(Y_observed[Z == 1]) - mean(Y_observed[Z == 0])\nprint(ATE_est)\n\n[1] 12.37752\n\n\nEven after changing the percentage of receiving treatment, it still suggests a good representation of the population.\nSimple random assignment ensures that the treatment and control groups are representative of the population, regardless of the percentage assigned to treatment.\n\n\nComplete random assignment\nComplete random assignment is very similar to simple random assignment, except that the researcher can specify exactly how many units are assigned to each condition.\nIf you only specify N, complete_ra() assigns exactly half of the subjects to treatment.\n\nZ &lt;- complete_ra(N = N)\ntable(Z)\n\nZ\n  0   1 \n300 300 \n\n\nTo change the number of units assigned, specify the m argument\n\nZ &lt;- complete_ra(N = N, m = 200)\ntable(Z)\n\nZ\n  0   1 \n400 200 \n\n\nIs Complete random assignment a good representation of the population?\n\n# Assign treatment and control groups based on random assignment\nsimulated_data2 &lt;- cbind(simulated_data,Z)\n\nY_observed &lt;- ifelse(simulated_data2$Z == 1, simulated_data2$Y1, simulated_data2$Y0)\n\nATE_est &lt;- mean(Y_observed[Z == 1]) - mean(Y_observed[Z == 0])\nprint(ATE_est)\n\n[1] 12.89898\n\n\nAgain, the estimated ATE is very close to the real ATE, which suggests a good representation of the population.\nWhen should you use simple_ra() versus complete_ra()? Basically, if the number of units is known beforehand, complete_ra() is always preferred, since researchers can plan exactly how many treatments will be deployed.\n\n\nBlock random assignment\nBlock random assignment, also known as stratified random assignment, is a powerful technique for improving the precision and interpretability of experimental results. In this design, subjects are first grouped into blocks based on pre-treatment characteristics, and then randomization occurs separately within each block. In our study, the blocking variable is Diet Type, which includes four categories: Vegetarian, Vegan, Omnivore, and Pescatarian. By blocking on diet type, we ensure that each dietary group has an approximately equal proportion of treated and control units, allowing for fair comparisons within each group. Blocking is particularly useful when we suspect that treatment effects may differ across diet types—for example, the treatment may have a stronger effect on one group than another.\nBlocking also enhances statistical precision when the blocking variable is correlated with the outcome. Since dietary habits may influence health-related outcomes, blocking on diet type helps reduce random variation and improves the accuracy of our treatment effect estimates.\nThe only required argument to block_ra() is blocks. Blocks can be a factor, character, or numeric variable. For example, when simulating data, we set DietType as a block.\n\nZ &lt;- block_ra(blocks = simulated_data$DietType)\ntable(Z, simulated_data$DietType)\n\n   \nZ   Vegetarian Vegan Keto Low-fat\n  0         84    60   82      74\n  1         85    60   81      74\n\n\nNow we see for each diet type, the subjects are devided equally into treatment and control group.\n\nZ &lt;- block_ra(N)\ntable(Z)\n\nZ\n0 \n1 \n\n\nIf we have multiple blocking variables – for example, DietType, FavoriteFruit, and PreferredDrink – we need to create a single composite blocking variable that uniquely identifies each combination of these three factors.\n\n# Create a composite block variable by pasting the three factors together\nsimulated_data$CompositeBlock &lt;- paste(simulated_data$DietType, simulated_data$FavoriteFruit, simulated_data$PreferredDrink, sep = \"_\")\n\n# Perform block random assignment using the composite block\nZ &lt;- block_ra(blocks = simulated_data$CompositeBlock)\n\n# Check treatment assignment within each block\nhead(table(Z, simulated_data$CompositeBlock))\n\n   \nZ   Keto_Apple_Juice Keto_Apple_Soda Keto_Apple_Tea Keto_Apple_Water\n  0                6               5              4                5\n  1                6               6              5                5\n   \nZ   Keto_Banana_Juice Keto_Banana_Soda Keto_Banana_Tea Keto_Banana_Water\n  0                 3                7               4                 8\n  1                 3                6               5                 9\n   \nZ   Keto_Cherry_Juice Keto_Cherry_Soda Keto_Cherry_Tea Keto_Cherry_Water\n  0                 4                6               3                 5\n  1                 5                6               3                 4\n   \nZ   Keto_Orange_Juice Keto_Orange_Soda Keto_Orange_Tea Keto_Orange_Water\n  0                 6                5               6                 3\n  1                 6                4               6                 4\n   \nZ   Low-fat_Apple_Juice Low-fat_Apple_Soda Low-fat_Apple_Tea\n  0                   4                  6                 4\n  1                   4                  6                 5\n   \nZ   Low-fat_Apple_Water Low-fat_Banana_Juice Low-fat_Banana_Soda\n  0                   4                    5                   5\n  1                   5                    5                   4\n   \nZ   Low-fat_Banana_Tea Low-fat_Banana_Water Low-fat_Cherry_Juice\n  0                  6                    5                    4\n  1                  5                    5                    4\n   \nZ   Low-fat_Cherry_Soda Low-fat_Cherry_Tea Low-fat_Cherry_Water\n  0                   7                  4                    3\n  1                   6                  5                    4\n   \nZ   Low-fat_Orange_Juice Low-fat_Orange_Soda Low-fat_Orange_Tea\n  0                    2                   6                  4\n  1                    2                   7                  4\n   \nZ   Low-fat_Orange_Water Vegan_Apple_Juice Vegan_Apple_Soda Vegan_Apple_Tea\n  0                    4                 2                5               5\n  1                    4                 3                5               6\n   \nZ   Vegan_Apple_Water Vegan_Banana_Juice Vegan_Banana_Soda Vegan_Banana_Tea\n  0                 3                  2                 4                3\n  1                 4                  2                 4                2\n   \nZ   Vegan_Banana_Water Vegan_Cherry_Juice Vegan_Cherry_Soda Vegan_Cherry_Tea\n  0                  2                  7                 3                4\n  1                  2                  6                 4                5\n   \nZ   Vegan_Cherry_Water Vegan_Orange_Juice Vegan_Orange_Soda Vegan_Orange_Tea\n  0                  4                  4                 3                4\n  1                  3                  5                 3                5\n   \nZ   Vegan_Orange_Water Vegetarian_Apple_Juice Vegetarian_Apple_Soda\n  0                  3                      5                     7\n  1                  3                      4                     6\n   \nZ   Vegetarian_Apple_Tea Vegetarian_Apple_Water Vegetarian_Banana_Juice\n  0                    6                      5                       5\n  1                    5                      5                       6\n   \nZ   Vegetarian_Banana_Soda Vegetarian_Banana_Tea Vegetarian_Banana_Water\n  0                      7                     8                       4\n  1                      6                     8                       5\n   \nZ   Vegetarian_Cherry_Juice Vegetarian_Cherry_Soda Vegetarian_Cherry_Tea\n  0                       6                      5                     6\n  1                       5                      4                     5\n   \nZ   Vegetarian_Cherry_Water Vegetarian_Orange_Juice Vegetarian_Orange_Soda\n  0                       4                       7                      4\n  1                       4                       6                      4\n   \nZ   Vegetarian_Orange_Tea Vegetarian_Orange_Water\n  0                     4                       5\n  1                     4                       4\n\n\nComparing the estimated ATE with the real ATE, it shows block random assignment is still representative of the population.\n\n# Assign treatment and control groups based on random assignment\nsimulated_data2 &lt;- cbind(simulated_data,Z)\n\nY_observed &lt;- ifelse(simulated_data2$Z == 1, simulated_data2$Y1, simulated_data2$Y0)\n\nATE_est &lt;- mean(Y_observed[Z == 1]) - mean(Y_observed[Z == 0])\nprint(ATE_est)\n\n[1] 12.4004\n\n\n\n\nClustered assignment\nClustered random assignment occurs when entire pre-existing groups (clusters) – rather than individuals – are assigned to treatment or control. This is sometimes unavoidable in experiments where treatment naturally occurs at the group level, such as assigning entire households, classrooms, or villages to an intervention.\nHowever, clustered assignment reduces the effective sample size, making it harder to detect treatment effects. If outcomes within a cluster are highly correlated (e.g., students in the same classroom perform similarly), the experiment effectively has fewer independent observations, reducing statistical power. In extreme cases, if outcomes are perfectly correlated within clusters, the experiment’s effective sample size is only equal to the number of clusters, not the number of individuals. Despite these drawbacks, clustered randomization remains necessary in many field experiments where individual-level randomization is impractical or could lead to contamination between treated and control units.\nIn R, clustered assignment can be implemented using the cluster_ra() function, ensuring that all units within a given cluster receive the same treatment while maintaining randomization integrity. For example, when simulating data, we set Region as a cluster.\n\nZ &lt;- cluster_ra(clusters = simulated_data$Region)\n\ntable(simulated_data$Region, Z)\n\n          Z\n             0   1\n  Region 1   0 111\n  Region 2   0  96\n  Region 3 113   0\n  Region 4  87   0\n  Region 5  88   0\n  Region 6   0 105\n\n\nThis shows that each cluster is either assigned to treatment or control. No two units within the same cluster are assigned to different conditions.\n\n\nBlocked and clustered assignment\nThe power of clustered experiments can sometimes be improved through blocking. In this scenario, whole clusters are members of a particular block – imagine villages nested within discrete regions, or classrooms nested within discrete schools.\nSince our data has Region to represent clustering and DietType to represent blocks, we can use block_and_cluster_ra() to conduct Blocked and clustered assignment.\n\n# Assign each Region to a single Diet Type (ensuring no overlap)\nregion_diet_mapping &lt;- aggregate(DietType ~ Region, data = simulated_data, FUN = function(x) unique(x)[1])\nsimulated_data &lt;- merge(simulated_data, region_diet_mapping, by = \"Region\", suffixes = c(\"\", \"_fixed\"))\nsimulated_data$DietType &lt;- simulated_data$DietType_fixed\nsimulated_data$DietType_fixed &lt;- NULL  # Remove extra column\n\nZ &lt;- block_and_cluster_ra(clusters = simulated_data$Region, blocks = simulated_data$DietType)\nhead(table(simulated_data$Region, Z))\n\n          Z\n             0   1\n  Region 1   0 111\n  Region 2  96   0\n  Region 3 113   0\n  Region 4  87   0\n  Region 5  88   0\n  Region 6   0 105\n\nhead(table(simulated_data$DietType, Z))\n\n            Z\n               0   1\n  Vegetarian  96   0\n  Vegan       88   0\n  Keto       113 111\n  Low-fat     87 105"
  },
  {
    "objectID": "Tutorial4.html",
    "href": "Tutorial4.html",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "",
    "text": "Mediation analysis is a key tool in causal inference, helping researchers understand how and why a treatment influences an outcome through an intermediate variable (mediator). In this tutorial, you’ll learn how to implement both traditional and causal mediation analysis in R using the mediation package, as well as explore structural equation modeling (SEM) with the lavaan package. By the end of this tutorial, you should be familiar with the following:\n1. packages mediation and lavaan\n2. Traditional mediation analysis\n3. Causal mediation analysis\n4. Structural equation modeling"
  },
  {
    "objectID": "Tutorial4.html#simple-model",
    "href": "Tutorial4.html#simple-model",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Simple model",
    "text": "Simple model\n\nresults &lt;- mediate(step2, step3, treat='X', mediator='M',boot=TRUE, sims=500)\n\nRunning nonparametric bootstrap\n\nsummary(results)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.3565       0.2178         0.52  &lt;2e-16 ***\nADE              0.0396      -0.1911         0.29   0.696    \nTotal Effect     0.3961       0.1624         0.66   0.004 ** \nProp. Mediated   0.9000       0.5019         2.01   0.004 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 500 \n\nplot(results)\n\n\n\n\n\n\n\n\nThe total effect in the summary is \\(b_1\\) in the fist step: total effect of X on Y without M. The direct effect (ADE) is \\(b_4\\) in the third step: a direct effect of X on Y after taking into account of a mediator effect of M. The mediation effect (ACME) is the total effect minus the direct effect (\\(b_1-b_4\\)), which equals to the product of a coefficient of X in the second step and a coefficient of M in the last step (\\(b_2*b_3\\)). The goal of mediation analysis is to obtain this mediation effect and see if it’s statistically significant.\nLet’s look into a more complicated model. We’re using job data in mediation package."
  },
  {
    "objectID": "Tutorial4.html#model-with-covariates",
    "href": "Tutorial4.html#model-with-covariates",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Model with covariates",
    "text": "Model with covariates\nWe use the jobs data from mediation package. This dataset is from the Job Search Intervention Study. In this dataset, people in the treatment group participated in job-skills workshops (treat). The outcome variable is a continuous measure of depressive symptoms (depress2). A continuous measure of job-search self efficacy represents a key mediating variable (job_seek). And we also have a list of covariates, including pre-treatment level of depression (depress1), education (educ), income, race(nonwhite), marital status (marrital), age, sex, previous occupation (occp), and the level of economic hardship (econ_hard).\n\ndata(\"jobs\")\nhead(jobs)\n\n  treat econ_hard depress1 sex      age                    occp marital\n1     1      3.00     1.91   1 34.16712           professionals married\n2     1      3.67     1.36   0 26.10137 operatives/kindred wrks nevmarr\n3     1      4.00     2.09   1 35.02192 operatives/kindred wrks nevmarr\n4     0      2.33     1.45   0 27.48767              manegerial married\n5     1      1.33     1.73   1 31.61096        clerical/kindred separtd\n6     1      3.00     1.55   0 40.43835              manegerial married\n    nonwhite   educ income job_seek depress2  work1 comply control job_dich\n1 non.white1 gradwk   50k+ 4.833333 1.727273 psyemp      0   treat        1\n2     white0 somcol 15t24k 3.833333 2.000000 psyemp      0   treat        0\n3 non.white1 somcol 25t39k 4.500000 2.181818 psyump      0   treat        1\n4     white0   bach 25t39k 3.666667 1.545455 psyump      0 control        0\n5 non.white1 highsc 25t39k 2.500000 2.363636 psyump      1   treat        0\n6     white0 highsc   50k+ 4.000000 1.181818 psyump      1   treat        1\n  job_disc\n1        4\n2        3\n3        4\n4        3\n5        2\n6        3\n\n\nFor causal mediation analysis, we first estimate two linear regressions for both the mediator and the outcome.\n\nmodel.m &lt;- lm(job_seek ~ treat + depress1 + econ_hard + sex + age + occp + marital + nonwhite + educ + income, data = jobs)\n\nmodel.y &lt;- lm(depress2 ~ treat + job_seek + depress1 + econ_hard + sex + age + occp + marital + nonwhite + educ + income, data = jobs)\n\nout.1 &lt;- mediate(model.m, model.y, sims = 1000, boot = TRUE, treat = \"treat\",\nmediator = \"job_seek\")\n\nRunning nonparametric bootstrap\n\nsummary(out.1)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value  \nACME            -0.0137      -0.0321         0.00   0.078 .\nADE             -0.0368      -0.1189         0.04   0.360  \nTotal Effect    -0.0505      -0.1338         0.02   0.204  \nProp. Mediated   0.2718      -1.9019         3.35   0.254  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 899 \n\n\nSimulations: 1000 \n\n\nThe job-skills workshop reduces depressive symptoms, at least in part, by increasing job-seeking behavior."
  },
  {
    "objectID": "Tutorial4.html#sensitivity",
    "href": "Tutorial4.html#sensitivity",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Sensitivity",
    "text": "Sensitivity\nWe can quickly run a sensitivity analysis using the medsens() function from the mediation package. This will help assess how unmeasured confounding between the mediator (job_seek) and outcome (depress2) could affect your results.\n\nsens.out &lt;- medsens(out.1, sims = 1000)\n\nsummary(sens.out)\n\n\nMediation Sensitivity Analysis for Average Causal Mediation Effect\n\nSensitivity Region\n\n       Rho    ACME 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n [1,] -0.9  0.1183      -0.0274       0.2640         0.81       0.5293\n [2,] -0.8  0.0715      -0.0166       0.1597         0.64       0.4182\n [3,] -0.7  0.0489      -0.0115       0.1093         0.49       0.3202\n [4,] -0.6  0.0342      -0.0081       0.0766         0.36       0.2353\n [5,] -0.5  0.0232      -0.0057       0.0520         0.25       0.1634\n [6,] -0.4  0.0142      -0.0038       0.0321         0.16       0.1046\n [7,] -0.3  0.0064      -0.0025       0.0153         0.09       0.0588\n [8,] -0.2 -0.0007      -0.0049       0.0036         0.04       0.0261\n [9,] -0.1 -0.0073      -0.0172       0.0026         0.01       0.0065\n[10,]  0.0 -0.0137      -0.0312       0.0037         0.00       0.0000\n[11,]  0.1 -0.0202      -0.0453       0.0050         0.01       0.0065\n[12,]  0.2 -0.0268      -0.0600       0.0065         0.04       0.0261\n[13,]  0.3 -0.0338      -0.0757       0.0080         0.09       0.0588\n[14,]  0.4 -0.0416      -0.0931       0.0098         0.16       0.1046\n[15,]  0.5 -0.0507      -0.1132       0.0118         0.25       0.1634\n[16,]  0.6 -0.0617      -0.1378       0.0144         0.36       0.2353\n[17,]  0.7 -0.0764      -0.1706       0.0177         0.49       0.3202\n[18,]  0.8 -0.0990      -0.2209       0.0229         0.64       0.4182\n[19,]  0.9 -0.1458      -0.3252       0.0337         0.81       0.5293\n\nRho at which ACME = 0: -0.2\nR^2_M*R^2_Y* at which ACME = 0: 0.04\nR^2_M~R^2_Y~ at which ACME = 0: 0.0261 \n\nplot(sens.out)\n\n\n\n\n\n\n\n\nρ (rho) represents the correlation between residual confounders that affect both the mediator (job_seek) and the outcome (depress2). ρ = -0.2 means that for the mediation effect (ACME) to completely disappear, an unmeasured confounder would need to create a correlation of -0.2 between the errors in M and Y. Since ρ = -0.2 is relatively small, the mediation effect is somewhat sensitive to hidden confounders.\nThe graph shows how fast ACME approaches 0 as ρ increases. If the ACME estimate does not change much across different rho values, it means that the mediation effect is not highly sensitive to hidden confounding. In contrast, if ACME drops to zero (or changes drastically) with a small shift in rho, it indicates that even a small amount of unmeasured confounding could invalidate the mediation effect."
  },
  {
    "objectID": "Tutorial4.html#mediator-is-also-moderator",
    "href": "Tutorial4.html#mediator-is-also-moderator",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Mediator is also moderator",
    "text": "Mediator is also moderator\nWe can also allow the causal mediation effect to vary with treatment status. Here, the model for the outcome must be altered by including an interaction term between the treatment indicator (treat) and the mediator variable (job_seek)\n\nmodel.y.inter &lt;- lm(depress2 ~ treat + job_seek + treat:job_seek + depress1 + econ_hard + sex + age + occp + marital + nonwhite + educ + income, data = jobs)\n\nout.2 &lt;- mediate(model.m, model.y.inter, sims = 1000, boot = TRUE, treat = \"treat\", mediator = \"job_seek\")\n\nRunning nonparametric bootstrap\n\nsummary(out.2)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n                         Estimate 95% CI Lower 95% CI Upper p-value\nACME (control)            -0.0185      -0.0451         0.00    0.10\nACME (treated)            -0.0117      -0.0298         0.00    0.10\nADE (control)             -0.0356      -0.1304         0.04    0.34\nADE (treated)             -0.0288      -0.1235         0.05    0.42\nTotal Effect              -0.0473      -0.1424         0.03    0.21\nProp. Mediated (control)   0.3920      -1.7387         3.50    0.26\nProp. Mediated (treated)   0.2482      -1.1572         2.35    0.26\nACME (average)            -0.0151      -0.0352         0.00    0.10\nADE (average)             -0.0322      -0.1251         0.04    0.38\nProp. Mediated (average)   0.3201      -1.3863         2.97    0.26\n\nSample Size Used: 899 \n\n\nSimulations: 1000 \n\n\nNow estimates for the mediation effects, direct effects and proportion of total effect mediated correspond to the levels of the treatment. In this case, the mediation effect under the treatment condition, listed as ACME (treated) is estimated to be −.012, while the mediation effect under the control condition, ACME (control), is −.019.\nBoth ACME values are negative, indicating that job-seeking behavior reduces depressive symptoms, regardless of whether individuals received treatment. However, the absolute value of ACME is larger for the control group (-0.0185) than for the treated group (-0.0117). This suggests that the indirect effect of job-seeking on depression is stronger when individuals did NOT receive treatment.\nWhen having a mediate object with interaction, we can select which treatment condition to plot the estimated effects for by selecting the treatment argument.\n\nplot(out.2, treatment = \"both\")\n\n\n\n\n\n\n\n#plot(out.2, treatment = \"treated\")\n#plot(out.2, treatment = \"control\")\n\nSolid lines represent treated group, and dotted lines represent control group. We can also make sensitivity plots for different treatment conditions.\n\nsens.out.2 &lt;- medsens(out.2, sims = 1000)\nsummary(sens.out.2)\n\n\nMediation Sensitivity Analysis: Average Mediation Effect\n\nSensitivity Region: ACME for Control Group\n\n       Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n [1,] -0.9        0.1133      -0.0264       0.2530         0.81       0.5279\n [2,] -0.8        0.0666      -0.0157       0.1489         0.64       0.4171\n [3,] -0.7        0.0441      -0.0107       0.0988         0.49       0.3194\n [4,] -0.6        0.0294      -0.0075       0.0662         0.36       0.2346\n [5,] -0.5        0.0183      -0.0054       0.0421         0.25       0.1629\n [6,] -0.4        0.0093      -0.0043       0.0230         0.16       0.1043\n [7,] -0.3        0.0015      -0.0061       0.0092         0.09       0.0587\n [8,] -0.2       -0.0055      -0.0155       0.0045         0.04       0.0261\n [9,] -0.1       -0.0121      -0.0288       0.0045         0.01       0.0065\n[10,]  0.0       -0.0185      -0.0425       0.0055         0.00       0.0000\n[11,]  0.1       -0.0250      -0.0566       0.0066         0.01       0.0065\n[12,]  0.2       -0.0316      -0.0712       0.0080         0.04       0.0261\n[13,]  0.3       -0.0386      -0.0868       0.0095         0.09       0.0587\n[14,]  0.4       -0.0464      -0.1040       0.0112         0.16       0.1043\n[15,]  0.5       -0.0554      -0.1240       0.0132         0.25       0.1629\n[16,]  0.6       -0.0664      -0.1486       0.0157         0.36       0.2346\n[17,]  0.7       -0.0811      -0.1813       0.0190         0.49       0.3194\n[18,]  0.8       -0.1037      -0.2316       0.0242         0.64       0.4171\n[19,]  0.9       -0.1504      -0.3357       0.0349         0.81       0.5279\n\nRho at which ACME for Control Group = 0: -0.3\nR^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\nR^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0587 \n\n\nSensitivity Region: ACME for Treatment Group\n\n       Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n [1,] -0.9        0.1201      -0.0278       0.2681         0.81       0.5279\n [2,] -0.8        0.0734      -0.0171       0.1639         0.64       0.4171\n [3,] -0.7        0.0509      -0.0119       0.1137         0.49       0.3194\n [4,] -0.6        0.0362      -0.0086       0.0809         0.36       0.2346\n [5,] -0.5        0.0251      -0.0062       0.0564         0.25       0.1629\n [6,] -0.4        0.0161      -0.0043       0.0366         0.16       0.1043\n [7,] -0.3        0.0083      -0.0030       0.0197         0.09       0.0587\n [8,] -0.2        0.0013      -0.0038       0.0064         0.04       0.0261\n [9,] -0.1       -0.0053      -0.0135       0.0029         0.01       0.0065\n[10,]  0.0       -0.0117      -0.0270       0.0035         0.00       0.0000\n[11,]  0.1       -0.0182      -0.0410       0.0047         0.01       0.0065\n[12,]  0.2       -0.0248      -0.0557       0.0061         0.04       0.0261\n[13,]  0.3       -0.0318      -0.0713       0.0077         0.09       0.0587\n[14,]  0.4       -0.0396      -0.0886       0.0094         0.16       0.1043\n[15,]  0.5       -0.0486      -0.1087       0.0114         0.25       0.1629\n[16,]  0.6       -0.0596      -0.1332       0.0139         0.36       0.2346\n[17,]  0.7       -0.0743      -0.1660       0.0173         0.49       0.3194\n[18,]  0.8       -0.0969      -0.2163       0.0225         0.64       0.4171\n[19,]  0.9       -0.1436      -0.3204       0.0332         0.81       0.5279\n\nRho at which ACME for Treatment Group = 0: -0.2\nR^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.04\nR^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0261 \n\nplot(sens.out.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mediation package is not limited to OLS models; it also supports mediation analysis for binary, ordinal, and other types of dependent variables, making it highly flexible for different study designs. For more details and documentation, visit the mediation package website."
  },
  {
    "objectID": "Tutorial4.html#parallel-mediation",
    "href": "Tutorial4.html#parallel-mediation",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Parallel mediation",
    "text": "Parallel mediation\nIn parallel mediation, multiple mediators operate independently to explain the effect of an independent variable (X) on an outcome (Y). This means that each mediator contributes separately to the total mediation effect, without influencing each other.\nWe first write down the model specifying all relationships. The indirect effects are defined to capture the mediation pathways: one through interest (mstr_int_achv = a × b) and another through anxiety (mstr_anxt_achv = c × d). The total effect combines all direct and indirect effects, allowing us to assess the full impact of mastery on achievement.\n\nmodel &lt;- \"achieve ~ b*interest + d*anxiety + e*mastery\n          interest ~ a*mastery\n          anxiety ~ c*mastery\n          mstr_int_achv := a*b\n          mstr_anxt_achv := c*d\n          Total := a*b + c*d + e\n\"\n\nIn lavaan, the := operator is used to define custom parameters based on existing model estimates. This is particularly useful in mediation analysis, where indirect effects need to be explicitly computed from path coefficients.\n\nfit &lt;- sem(model, data = SEMdata, se=\"bootstrap\", bootstrap = 1000)\n\nWarning: lavaan-&gt;lav_model_nvcov_bootstrap():  \n   1 bootstrap runs failed or did not converge.\n\nsummary(fit, fit.measures = T, standardized = T, rsquare = T)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                           140\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.010\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.920\n\nModel Test Baseline Model:\n\n  Test statistic                               149.860\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.041\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -704.422\n  Loglikelihood unrestricted model (H1)       -704.417\n                                                      \n  Akaike (AIC)                                1424.844\n  Bayesian (BIC)                              1448.377\n  Sample-size adjusted Bayesian (SABIC)       1423.066\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.082\n  P-value H_0: RMSEA &lt;= 0.050                    0.933\n  P-value H_0: RMSEA &gt;= 0.080                    0.051\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.002\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws             999\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  achieve ~                                                             \n    interest   (b)    0.211    0.072    2.935    0.003    0.211    0.294\n    anxiety    (d)   -0.040    0.056   -0.722    0.470   -0.040   -0.053\n    mastery    (e)    0.345    0.096    3.591    0.000    0.345    0.379\n  interest ~                                                            \n    mastery    (a)    0.770    0.083    9.319    0.000    0.770    0.607\n  anxiety ~                                                             \n    mastery    (c)   -0.399    0.090   -4.436    0.000   -0.399   -0.337\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .achieve           0.988    0.102    9.737    0.000    0.988    0.613\n   .interest          1.973    0.229    8.609    0.000    1.973    0.631\n   .anxiety           2.415    0.211   11.439    0.000    2.415    0.886\n\nR-Square:\n                   Estimate\n    achieve           0.387\n    interest          0.369\n    anxiety           0.114\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    mstr_int_achv     0.162    0.057    2.856    0.004    0.162    0.178\n    mstr_anxt_achv    0.016    0.023    0.692    0.489    0.016    0.018\n    Total             0.524    0.067    7.776    0.000    0.524    0.575\n\n\nResults indicate that mastery goal has indirect effect of 0.162 on achievement through interest. But mastery goal doesn’t have indirect effect on achievement through anxiety. Mastery goal has total effect of 0.524 on achievement."
  },
  {
    "objectID": "Tutorial4.html#sequential-mediation",
    "href": "Tutorial4.html#sequential-mediation",
    "title": "Tutorial 4 Mediation Analysis",
    "section": "Sequential mediation",
    "text": "Sequential mediation\nIn sequential mediation, the mediators are causally dependent on each other, forming a chain-like process where the effect of the independent variable (X) is transmitted through multiple steps before reaching the outcome (Y). Unlike parallel mediation, where mediators operate independently, sequential mediation assumes that one mediator influences another before affecting Y. This allows researchers to explore how intermediate mechanisms unfold over time. For example, in an educational setting, mastery might first enhance interest, which then reduces anxiety, ultimately leading to higher achievement.\n\nmodel2 &lt;- \"achieve ~ c*anxiety + e*interest + f*mastery\n           interest ~ a*mastery\n           anxiety ~ b*interest + d*mastery\n           mstr_int_anxt_achv := a*b*c\n           mstr_int_achv := a*e\n           mstr_anxt_achv := d*c\n           int_anxt_achv := b*c\n           total_mstr := a*b*c + a*e + d*c + f\n           total_int := b*c + e\n\"\n\nNote that in sequential mediation, it is not enough to only compute the total and indirect effects of the initial treatment variable (mastery). Since mediators influence one another in a chain, we must also consider the total and indirect effects of the first mediator (interest) to fully capture the mediation process.\n\nfit2 &lt;- sem(model2, data = SEMdata, se=\"bootstrap\", bootstrap = 1000)\nsummary(fit2, fit.measures = T, standardized = T, rsquare = T)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                           140\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                               149.860\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -704.417\n  Loglikelihood unrestricted model (H1)       -704.417\n                                                      \n  Akaike (AIC)                                1426.834\n  Bayesian (BIC)                              1453.309\n  Sample-size adjusted Bayesian (SABIC)       1424.834\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws            1000\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  achieve ~                                                             \n    anxiety    (c)   -0.040    0.056   -0.723    0.470   -0.040   -0.053\n    interest   (e)    0.211    0.072    2.933    0.003    0.211    0.294\n    mastery    (f)    0.345    0.096    3.592    0.000    0.345    0.379\n  interest ~                                                            \n    mastery    (a)    0.770    0.083    9.310    0.000    0.770    0.607\n  anxiety ~                                                             \n    interest   (b)    0.009    0.099    0.095    0.924    0.009    0.010\n    mastery    (d)   -0.406    0.116   -3.508    0.000   -0.406   -0.343\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .achieve           0.988    0.101    9.741    0.000    0.988    0.613\n   .interest          1.973    0.229    8.606    0.000    1.973    0.631\n   .anxiety           2.415    0.212   11.382    0.000    2.415    0.886\n\nR-Square:\n                   Estimate\n    achieve           0.387\n    interest          0.369\n    anxiety           0.114\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    mstr_nt_nxt_ch   -0.000    0.006   -0.052    0.958   -0.000   -0.000\n    mstr_int_achv     0.162    0.057    2.857    0.004    0.162    0.178\n    mstr_anxt_achv    0.016    0.023    0.701    0.484    0.016    0.018\n    int_anxt_achv    -0.000    0.007   -0.053    0.958   -0.000   -0.001\n    total_mstr        0.524    0.067    7.778    0.000    0.524    0.575\n    total_int         0.211    0.071    2.952    0.003    0.211    0.293\n\n\nResults indicate that mastery goal has indirect effect of 0.162 on achievement through interest. But this indirect effect does not happen through anxiety. Mastery goal also show a total effect of 0.524 on achievement."
  }
]