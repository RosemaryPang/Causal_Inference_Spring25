[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome Page",
    "section": "",
    "text": "Welcome to DACSS790 Causal Inference. All classroom material, including recordings, slides, tutorials, and assignment descriptions, will be posted in Canvas.\nThis tutorial website contains the R code and output of weekly tutorials.\n Weekly Tutorials \nWeek 1 Regression Review & Bias\nWeek 2 Directed Acyclic Graphs"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "",
    "text": "In this tutorial, you’ll learn (or refresh your memory) about hypothesis testing, ordinary least squares (OLS) regression, transformation, logistic regression, and we’ll discuss why we can’t make causal statement by simply adding control variables to our model.\nBy the end of this tutorial, you should be familiar with the following:\n1. Basic concepts of hypothesis testing\n2. OLS regression: single and multiple linear regression, regression with interaction term\n3. Transformation\n4. Logistic regression"
  },
  {
    "objectID": "Tutorial1.html#example",
    "href": "Tutorial1.html#example",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Example",
    "text": "Example\nA manufacturer says the battery of their laptop lasts 10 hours on average. We want to test whether this is true (\\(\\mu = 10\\)). Because we are interested in deviations from 10 in either direction, this is a two-sided test. Now we have a sample, with sample mean (\\(\\bar{x}\\)) = 9.04, sample standard deviation (\\({s}\\)) = 1.83, and sample size = 30. Can we reject the null hypothesis?\nWe first calculate t-value based on sample statistics:\n\\[t=\\frac{\\bar{x} - \\mu_0}{{s}/{\\sqrt{n}}}=\\frac{9.04-10}{1.83/{\\sqrt{30}}}=-2.87\\]\nWe then need to find the critical value base on our \\(\\alpha\\)-level. Let’s set \\(\\alpha=0.05\\). Since the absolute value of t-value is greater than the critical value, we can reject the null hypothesis.\n\ncritical &lt;- round(qt(0.975,df=(30-1)),2)\ncritical\n\n[1] 2.05\n\n\nWe can manually calculate the p-value. With a p-value smaller than 0.05 (the \\(\\alpha\\)-level), we can also reject the null hypothesis.\n\nt_statistic &lt;- 2.87\np_value &lt;- (1 - pt(t_statistic, df= 29)) * 2\np_value\n\n[1] 0.00758546"
  },
  {
    "objectID": "Tutorial1.html#simple-linear-regression",
    "href": "Tutorial1.html#simple-linear-regression",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nTo start, let’s consider the simplest case—a single independent variable predicting an outcome. This is known as simple linear regression, where we model a straight-line relationship between the predictor and the response variable. We will walk through an example using student.survey data.\n\ndata(student.survey)\nhead(student.survey)\n\n  subj ge ag  hi  co   dh   dr tv sp ne ah    ve pa           pi           re\n1    1  m 32 2.2 3.5    0  5.0  3  5  0  0 FALSE  r conservative   most weeks\n2    2  f 23 2.1 3.5 1200  0.3 15  7  5  6 FALSE  d      liberal occasionally\n3    3  f 27 3.3 3.0 1300  1.5  0  4  3  0 FALSE  d      liberal   most weeks\n4    4  f 35 3.5 3.2 1500  8.0  5  5  6  3 FALSE  i     moderate occasionally\n5    5  m 23 3.1 3.5 1600 10.0  6  6  3  0 FALSE  i very liberal        never\n6    6  m 39 3.5 3.5  350  3.0  4  5  7  0 FALSE  d      liberal occasionally\n     ab    aa    ld\n1 FALSE FALSE FALSE\n2 FALSE FALSE    NA\n3 FALSE FALSE    NA\n4 FALSE FALSE FALSE\n5 FALSE FALSE FALSE\n6 FALSE FALSE    NA\n\nstudent.survey$male &lt;- ifelse(student.survey$ge == \"m\", 1, 0)\n\nThis data is from smss package. It consists of responses of graduate students in the social sciences enrolled in STA 6126 in a recent term at the University of Florida. Variables in this data include gender (ge), high school GPA (hi), average number of hours per week that you watch TV (tv), and political affiliation (pa). We start by looking into the relationship between watching TV and high school GPA.\n\nggplot(data = student.survey, aes(x = tv, y = hi)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Hours of TV Watched per Week\", \n       y = \"GPA\", \n       title = \"Relationship Between TV Watching and GPA\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe visualization tells us there’s a negative relationship between watching TV and GPA. We can use OLS regression to test whether this relationship is statistically significant.\n\nm1 &lt;- lm(hi~tv, data=student.survey)\nsummary(m1)\n\n\nCall:\nlm(formula = hi ~ tv, data = student.survey)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2583 -0.2456  0.0417  0.3368  0.7051 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.441353   0.085345  40.323   &lt;2e-16 ***\ntv          -0.018305   0.008658  -2.114   0.0388 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4467 on 58 degrees of freedom\nMultiple R-squared:  0.07156,   Adjusted R-squared:  0.05555 \nF-statistic: 4.471 on 1 and 58 DF,  p-value: 0.03879\n\n\nFrom the OLS regression results, we can model the relationship between hours watching TV and high school GPA: \\(hi=3.441−0.0183*tv\\). When a student watches 0 hours of TV, their predicted GPA is 3.441. Each additional hour of TV watched per week is associated with a decrease of 0.018 points in the GPA, and the impact is statistically significant at the 95% confidence level."
  },
  {
    "objectID": "Tutorial1.html#multiple-linear-regression",
    "href": "Tutorial1.html#multiple-linear-regression",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn reality, most outcomes are influenced by more than one factor. Simple linear regression is often too simplistic to capture complex relationships. Multiple linear regression extends the model by including multiple independent variables, allowing us to control for additional factors and better isolate the effect of each predictor. Let’s explore how this works with the same data.\n\nm2 &lt;- lm(hi~tv+male+pa,data=student.survey)\nsummary(m2)\n\n\nCall:\nlm(formula = hi ~ tv + male + pa, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.11908 -0.25376  0.02283  0.31597  0.68353 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.428977   0.133216  25.740   &lt;2e-16 ***\ntv          -0.017175   0.009004  -1.907   0.0617 .  \nmale        -0.138154   0.117482  -1.176   0.2447    \npai          0.161780   0.134692   1.201   0.2349    \npar          0.024887   0.153392   0.162   0.8717    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.446 on 55 degrees of freedom\nMultiple R-squared:  0.1223,    Adjusted R-squared:  0.0585 \nF-statistic: 1.917 on 4 and 55 DF,  p-value: 0.1206\n\n\nResults show that after controlling for gender and party affiliation, hours of TV watching still has a statistically significant impact on GPA at the 90% confidence level. Each additional hour of TV watched per week is associated with a 0.017 point decrease in GPA. Compared to Democrats, Independents have a 0.16 point higher GPA, but this difference is not statistically significant.\nIn this model, we use Democrats as the reference category (baseline) for party affiliation. You can also change the baseline into other category using relevel() function.\n\nstudent.survey$pa2 &lt;- relevel(student.survey$pa, ref = \"i\")\n\nIf we set Independents as the baseline category for party affiliation, how does being Republican affect high school GPA compared to Independents?"
  },
  {
    "objectID": "Tutorial1.html#interaction-term",
    "href": "Tutorial1.html#interaction-term",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Interaction Term",
    "text": "Interaction Term\nSo far, we have assumed that each independent variable has an independent and additive effect on the dependent variable. However, in many cases, the effect of one variable depends on the level of another. This is where interaction terms come in. By including interaction terms in our regression model, we can capture how the relationship between one variable and the outcome changes depending on another variable.\n\nm3 &lt;- lm(hi~tv*male, data=student.survey)\nsummary(m3)\n\n\nCall:\nlm(formula = hi ~ tv * male, data = student.survey)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18709 -0.23932  0.07162  0.30540  0.74298 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.539245   0.130413  27.139   &lt;2e-16 ***\ntv          -0.021885   0.012852  -1.703   0.0941 .  \nmale        -0.177346   0.173034  -1.025   0.3098    \ntv:male      0.004405   0.017531   0.251   0.8025    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4482 on 56 degrees of freedom\nMultiple R-squared:  0.09761,   Adjusted R-squared:  0.04926 \nF-statistic: 2.019 on 3 and 56 DF,  p-value: 0.1216\n\n\nIn this model, \\(GPA=3.539−0.0219TV−0.1773Male+0.0044(TV*Male)\\). Unlike previous models where we could interpret coefficients directly, interaction terms modify the effect of one variable depending on the value of another. This means we need to substitute specific values to understand the impact on different groups. For female students (male=0), watching one additional hour of TV is associated with a 0.0219 point decrease in GPA. For male students (male=1), each additional hour of TV is associated with a 0.0175 point (\\(-0.219+0.044\\)) decrease in GPA. Since the interaction term is not statistically significant, it suggests that the relationship between TV watching and GPA is similar for both genders."
  },
  {
    "objectID": "Tutorial1.html#regression-table",
    "href": "Tutorial1.html#regression-table",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Regression Table",
    "text": "Regression Table\nTo present the results of the three regression models (m1, m2, and m3) side by side in a well-formatted table, you can use the stargazer package in R.\n\nstargazer(m1, m2, m3, type = \"text\",\n          title = \"Regression Results\",\n          dep.var.labels = \"High School GPA\",\n          covariate.labels = c(\"TV Hours\", \"Male\", \"Party Affiliation: Independent\", \"Party Affiliation: Republican\", \"TV Hours x Male\"),\n          omit.stat = c(\"f\", \"ser\"),\n          no.space = TRUE)\n\n\nRegression Results\n============================================================\n                                    Dependent variable:     \n                               -----------------------------\n                                      High School GPA       \n                                  (1)       (2)       (3)   \n------------------------------------------------------------\nTV Hours                       -0.018**   -0.017*   -0.022* \n                                (0.009)   (0.009)   (0.013) \nMale                                      -0.138    -0.177  \n                                          (0.117)   (0.173) \nParty Affiliation: Independent             0.162            \n                                          (0.135)           \nParty Affiliation: Republican              0.025            \n                                          (0.153)           \nTV Hours x Male                                      0.004  \n                                                    (0.018) \nConstant                       3.441***  3.429***  3.539*** \n                                (0.085)   (0.133)   (0.130) \n------------------------------------------------------------\nObservations                      60        60        60    \nR2                               0.072     0.122     0.098  \nAdjusted R2                      0.056     0.059     0.049  \n============================================================\nNote:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "Tutorial1.html#simpsons-paradox-and-collider-bias",
    "href": "Tutorial1.html#simpsons-paradox-and-collider-bias",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Simpson’s Paradox and Collider Bias",
    "text": "Simpson’s Paradox and Collider Bias\nSimpson’s paradox means that a tend appears in different groups of data, but disappears or reverses when these groups are combined. Collider bias occurs when we condition on (control for, subset by, or select a sample based on) a variable that is influenced by two other variables. Let’s look into an example using simulated data.\n\nset.seed(123)\n\nn &lt;- 500  \n\n# Generate exercise levels (hours per week)\nexercise &lt;- rnorm(n, mean = 5, sd = 2)\n\n# Generate blood pressure - Randomly generated, NO relationship with exercise\nblood_pressure &lt;- rnorm(n, mean = 120, sd = 5)\n\n# Obesity is influenced by both exercise and blood pressure (collider)\nobesity &lt;- ifelse(exercise &lt; 5 | blood_pressure &gt; 120, 1, 0)\n\n# Modify Blood Pressure to create group-specific effects\nblood_pressure[obesity == 0] &lt;- blood_pressure[obesity == 0] - 2 * exercise[obesity == 0]  # Exercise LOWERS BP for non-obese\nblood_pressure[obesity == 1] &lt;- blood_pressure[obesity == 1] + 2 * exercise[obesity == 1]  # Exercise RAISES BP for obese\n\n# Create dataframe\ndata &lt;- data.frame(exercise, blood_pressure, obesity)\n\nThe visualization below shows an example of Simpson’s Paradox: the trend differs when analyzing separate groups compared to the pooled data.\n\nggplot(data, aes(x = exercise, y = blood_pressure, color = as.factor(obesity))) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"Pooled Data\"), method = \"lm\", se = FALSE, data = data, linetype = \"dashed\") +\n  labs(title = \"Simpson's Paradox: Exercise & Blood Pressure with Obesity as Collider\",\n       x = \"Exercise (hours per week)\",\n       y = \"Blood Pressure (mmHg)\",\n       color = \"Obesity Status\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn our case, obesity is a collider because it is influenced by both exercise and blood pressure. Let’s examine the regression results by running models on the pooled data, within each obesity group separately, and with obesity as a control variable.\n\n# Overall regression (ignoring obesity)\noverall &lt;- lm(blood_pressure ~ exercise, data = data)\n\n# Regression within subgroup\nobese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 1))\nnon_obese &lt;- lm(blood_pressure ~ exercise, data = data %&gt;% filter(obesity == 0))\n\n# Controlling for obesity\ncontrol &lt;- lm(blood_pressure ~ exercise + obesity, data = data)\n\nstargazer(overall, obese, non_obese, control,\n          type = \"text\")\n\n\n=======================================================================================================================\n                                                            Dependent variable:                                        \n                    ---------------------------------------------------------------------------------------------------\n                                                              blood_pressure                                           \n                              (1)                     (2)                      (3)                      (4)            \n-----------------------------------------------------------------------------------------------------------------------\nexercise                   -1.593***                2.819***                -1.997***                 2.048***         \n                            (0.314)                 (0.128)                  (0.211)                  (0.135)          \n                                                                                                                       \nobesity                                                                                              32.395***         \n                                                                                                      (0.586)          \n                                                                                                                       \nConstant                  130.907***               117.935***              115.721***                88.997***         \n                            (1.705)                 (0.619)                  (1.422)                  (0.991)          \n                                                                                                                       \n-----------------------------------------------------------------------------------------------------------------------\nObservations                  500                     362                      138                      500            \nR2                           0.049                   0.575                    0.396                    0.867           \nAdjusted R2                  0.047                   0.573                    0.392                    0.866           \nResidual Std. Error    13.651 (df = 498)        4.443 (df = 360)        3.206 (df = 136)          5.112 (df = 497)     \nF Statistic         25.717*** (df = 1; 498) 486.228*** (df = 1; 360) 89.334*** (df = 1; 136) 1,618.847*** (df = 2; 497)\n=======================================================================================================================\nNote:                                                                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIn Model 1, we see that exercise has a negative impact on blood pressure, which makes sense. However, when we examine different subgroups, we find that the direction of the effect varies across groups—a classic example of Simpson’s Paradox. Surprisingly, in Model 4, after controlling for obesity, the coefficient for exercise flips from negative to positive. Does this mean that exercising regularly increases blood pressure? Of course not. This misleading result occurs because obesity is a collider, and controlling for a collider introduces bias rather than reducing it."
  },
  {
    "objectID": "Tutorial1.html#multicollinearity",
    "href": "Tutorial1.html#multicollinearity",
    "title": "Tutorial 1 Regression Review & Bias",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nAdding too many control variables can lead to multicollinearity, where some predictors become highly correlated with each other. This makes it difficult to determine the independent effect of each variable, leading to unstable estimates and inflated standard errors. As a result, even if a variable has a real effect, its coefficient may appear insignificant due to the overlap in explanatory power with other controls. Let’s look into the example below.\nIn this example, we have 100 employee with employees’ ages range from 25 to 65. Everyone in the company graduated college at the age of 22, and started working there. Everyone is paid roughly based on their experience. If we run regression using both age and experience as independent variables, we have multicollinearity problem.\n\nset.seed(123)\nage &lt;- sample(25:65, size  = 100, replace = TRUE)\nexperience &lt;- age - 22\nsalary &lt;- 2000 * (experience) + rnorm(100, sd = 20000)\n\nsummary(lm(salary ~ age + experience))\n\n\nCall:\nlm(formula = salary ~ age + experience)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40598 -12992   -983  11339  65231 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -41856.8     8607.8  -4.863 4.41e-06 ***\nage           1943.2      181.7  10.694  &lt; 2e-16 ***\nexperience        NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19630 on 98 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.5338 \nF-statistic: 114.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThis situation is easy to detect because R completely omits the coefficient for experience, indicating perfect multicollinearity. However, in real-world cases, variables may be highly correlated but not perfectly related, meaning R will still report a coefficient. For example, let’s add some variation to age.\n\nage &lt;- age + round(rnorm(100)) \nsummary(lm(salary ~ age + experience))\n\n\nCall:\nlm(formula = salary ~ age + experience)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40591 -12986   -992  11336  65225 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   757.713  45204.994   0.017    0.987\nage             6.244   2049.783   0.003    0.998\nexperience   1936.977   2066.586   0.937    0.351\n\nResidual standard error: 19740 on 97 degrees of freedom\nMultiple R-squared:  0.5385,    Adjusted R-squared:  0.529 \nF-statistic: 56.59 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nNow, we see that although R reports coefficients for both variables, the results are quite strange. The standard errors are extremely high. The p-values are large, meaning we fail to detect a significant relationship – even though age and experience should clearly influence salary. However, the F-test p-value is still small, suggesting that at least one of the predictors matters – we just can’t tell which one!\nThis demonstrates two key indicators of multicollinearity: 1. Large standard errors (high uncertainty in estimates). 2. High p-values for individual variables, despite a significant overall model (low F-test p-value).\nMulticollinearity makes estimates unreliable, leading to misleading or biased results. So, be cautious when adding too many highly correlated control variables—sometimes, they do more harm than good!"
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "",
    "text": "Directed Acyclic Graphs (DAGs) help us visualize causal relationships between variables. In this tutorial, you’ll learn about creating DAGs in R. By the end of this tutorial, you should be familiar with the following:\n1. packages ggdag and dagitty\n2. Create a simple DAG\n3. Customize DAG with colors, and labels\n4. Highlight causal paths, confounders, and colliders"
  },
  {
    "objectID": "Tutorial2.html#finding-paths",
    "href": "Tutorial2.html#finding-paths",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Finding paths",
    "text": "Finding paths\nggdag_paths() function shows all the causal and non-causal paths between the exposure and outcome, including frontdoor paths, backdoor paths and paths involving colliders.\n\nggdag_paths(winelife)"
  },
  {
    "objectID": "Tutorial2.html#finding-parents",
    "href": "Tutorial2.html#finding-parents",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Finding parents",
    "text": "Finding parents\nThe ggdag_parents() function identifies and visualizes the parent nodes of a selected variable in a DAG. Identifying parent nodes can help identify potential backdoor paths if the selected node is the exposure in a DAG.\n\nggdag_parents(winelife_label, \"Wine\", text=F, use_labels = \"label\")"
  },
  {
    "objectID": "Tutorial2.html#closing-the-backdoor",
    "href": "Tutorial2.html#closing-the-backdoor",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Closing the backdoor",
    "text": "Closing the backdoor\nThe backdoor criterion says that we have sufficient set of variables to control for confounding when it blocks all backdoor paths from treatment to the outcome, and when it does not include any descendants of treatment. Using ggdag_adjustment_set() function, we can quickly get the minimally sufficient adjustment sets to adjust for when analyzing the effect of x on y.\n\nggdag_adjustment_set(winelife_label,text=F,use_labels = \"label\")\n\n\n\n\n\n\n\n\nLet’s see another practice using the example on Lecture Slide page 23.\n\n# First make the DAG\nPractice &lt;- dagify(\n  Y ~ V + A + M,\n  A ~ Z + W,\n  M ~ W,\n  W ~ Z,\n  Z ~ V,\n  exposure = \"A\",\n  outcome = \"Y\"\n)\n\nggdag(Practice, layout=\"stress\")\n\n\n\n\n\n\n\nggdag_adjustment_set(Practice)"
  },
  {
    "objectID": "Tutorial2.html#control-for-collider",
    "href": "Tutorial2.html#control-for-collider",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Control for collider?",
    "text": "Control for collider?\nIn the lecture, we emphasized that we should never control/block a collider. Because doing so induces a fake correlation between its parent variables, even if they were originally independent.\nTo formally analyze whether variables are conditionally independent (d-separated) or conditionally related (d-connected) in a DAG, we can use the function ggdag_dseparated().\n\n# First create a DAG with a collider\nIncomeHealth &lt;- dagify(\n  Wine ~ Income + Health,\n  exposure = \"Income\",\n  outcome = \"Health\"\n)\n\nggdag(IncomeHealth, layout='tree')\n\n\n\n\n\n\n\nggdag_dseparated(IncomeHealth)\n\n\n\n\n\n\n\n\nWe see that Health and Income are d-separated, meaning they are independent. Let’s see what happens if we control for the collider.\n\nggdag_dseparated(IncomeHealth,\n  controlling_for = \"Wine\")\n\n\n\n\n\n\n\n\nNow we see after controlling the collider, Health and Income become d-connected, meaning we are creating a fake relationship between them."
  },
  {
    "objectID": "Tutorial2.html#blocking-the-frontdoor",
    "href": "Tutorial2.html#blocking-the-frontdoor",
    "title": "Tutorial 2 Directed Acyclic Graphs",
    "section": "Blocking the frontdoor?",
    "text": "Blocking the frontdoor?\nFrontdoor adjustment leverages the mediator as a tool to estimate the causal effect of X on Y. It does not simply ‘control for’ or ‘block’ the mediator in the same way we adjust for confounder. We will discuss frontdoor adjustment in more detail during the Mediation Analysis week. For now, let’s explore what happens when we block the mediator.\n\n# First create a DAG with a mediator\nwinedrug &lt;- dagify(\n  Lifespan ~ Drug,\n  Drug ~ Wine,\n  exposure = \"Wine\",\n  outcome = \"Lifespan\"\n)\n\nggdag(winedrug, layout='stress')\n\n\n\n\n\n\n\nggdag_dseparated(winedrug)\n\n\n\n\n\n\n\nggdag_dseparated(winedrug, controlling_for = \"Drug\")"
  }
]